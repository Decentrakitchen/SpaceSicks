{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Advanced Multiclass Classification Pipeline for tfopwg_disp\n",
        "============================================================\n",
        "This module implements a production-ready machine learning solution for\n",
        "multiclass classification with comprehensive data processing, feature engineering,\n",
        "advanced modeling, and detailed evaluation.\n",
        "\n",
        "Author: ML Pipeline\n",
        "Date: October 4, 2025\n",
        "\"\"\"\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "\n",
        "# Models\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, accuracy_score,\n",
        "    precision_recall_fscore_support, roc_auc_score, roc_curve,\n",
        "    matthews_corrcoef, cohen_kappa_score, log_loss\n",
        ")\n",
        "\n",
        "# Utilities\n",
        "import joblib\n",
        "from scipy import stats\n",
        "from collections import Counter\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "# Styling\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "\n",
        "class DataProcessor:\n",
        "    \"\"\"Handle data loading, cleaning, and preprocessing\"\"\"\n",
        "\n",
        "    def __init__(self, filepath):\n",
        "        self.filepath = filepath\n",
        "        self.df = None\n",
        "        self.target_col = 'tfopwg_disp'\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load and perform initial data inspection\"\"\"\n",
        "        print(\"=\"*70)\n",
        "        print(\"LOADING DATA\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        self.df = pd.read_csv(self.filepath)\n",
        "        print(f\"\\nâœ“ Data loaded successfully: {self.df.shape[0]} rows, {self.df.shape[1]} columns\")\n",
        "\n",
        "        # Display target distribution\n",
        "        print(f\"\\nðŸ“Š Target Variable Distribution ({self.target_col}):\")\n",
        "        print(self.df[self.target_col].value_counts())\n",
        "        print(f\"\\nTarget Class Proportions:\")\n",
        "        print(self.df[self.target_col].value_counts(normalize=True).round(4))\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def analyze_data_quality(self):\n",
        "        \"\"\"Comprehensive data quality analysis\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"DATA QUALITY ANALYSIS\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Missing values\n",
        "        missing = self.df.isnull().sum()\n",
        "        missing_pct = (missing / len(self.df) * 100).round(2)\n",
        "        missing_df = pd.DataFrame({\n",
        "            'Missing_Count': missing,\n",
        "            'Missing_Percentage': missing_pct\n",
        "        }).sort_values('Missing_Percentage', ascending=False)\n",
        "\n",
        "        print(\"\\nðŸ“‹ Columns with Missing Values:\")\n",
        "        print(missing_df[missing_df['Missing_Count'] > 0].head(20))\n",
        "\n",
        "        # Data types\n",
        "        print(\"\\nðŸ“ Data Types:\")\n",
        "        print(self.df.dtypes.value_counts())\n",
        "\n",
        "        # Numeric vs categorical\n",
        "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        categorical_cols = self.df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "        print(f\"\\nâœ“ Numeric columns: {len(numeric_cols)}\")\n",
        "        print(f\"âœ“ Categorical columns: {len(categorical_cols)}\")\n",
        "\n",
        "        return missing_df, numeric_cols, categorical_cols\n",
        "\n",
        "    def prepare_features_target(self):\n",
        "        \"\"\"Separate features and target, handle data types\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"PREPARING FEATURES AND TARGET\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Remove non-predictive columns\n",
        "        cols_to_drop = ['toi', 'tid', 'rastr', 'decstr', 'toi_created', 'rowupdate', self.target_col]\n",
        "        feature_cols = [col for col in self.df.columns if col not in cols_to_drop]\n",
        "\n",
        "        X = self.df[feature_cols].copy()\n",
        "        y = self.df[self.target_col].copy()\n",
        "\n",
        "        # Encode target\n",
        "        y_encoded = self.label_encoder.fit_transform(y)\n",
        "\n",
        "        print(f\"\\nâœ“ Features shape: {X.shape}\")\n",
        "        print(f\"âœ“ Target shape: {y_encoded.shape}\")\n",
        "        print(f\"âœ“ Number of classes: {len(self.label_encoder.classes_)}\")\n",
        "        print(f\"âœ“ Classes: {list(self.label_encoder.classes_)}\")\n",
        "\n",
        "        return X, y_encoded, feature_cols\n",
        "\n",
        "\n",
        "class FeatureEngineer:\n",
        "    \"\"\"Advanced feature engineering and selection\"\"\"\n",
        "\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.imputer = None\n",
        "        self.scaler = None\n",
        "        self.selected_features = None\n",
        "\n",
        "    def handle_missing_values(self, strategy='advanced'):\n",
        "        \"\"\"Handle missing values with multiple strategies\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"HANDLING MISSING VALUES\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        missing_cols = self.X.columns[self.X.isnull().any()].tolist()\n",
        "\n",
        "        if len(missing_cols) == 0:\n",
        "            print(\"\\nâœ“ No missing values detected\")\n",
        "            return self.X\n",
        "\n",
        "        # Remove columns with too many missing values (>95%)\n",
        "        missing_pct = (self.X.isnull().sum() / len(self.X) * 100)\n",
        "        cols_to_drop = missing_pct[missing_pct > 95].index.tolist()\n",
        "\n",
        "        if len(cols_to_drop) > 0:\n",
        "            print(f\"\\nðŸ—‘ï¸  Dropping {len(cols_to_drop)} columns with >95% missing values:\")\n",
        "            for col in cols_to_drop:\n",
        "                print(f\"  â€¢ {col} ({missing_pct[col]:.1f}% missing)\")\n",
        "            self.X = self.X.drop(columns=cols_to_drop)\n",
        "            missing_cols = self.X.columns[self.X.isnull().any()].tolist()\n",
        "\n",
        "        if len(missing_cols) == 0:\n",
        "            print(\"\\nâœ“ No missing values remaining after dropping high-missing columns\")\n",
        "            return self.X\n",
        "\n",
        "        if strategy == 'advanced':\n",
        "            # Use KNN imputer for better imputation\n",
        "            print(f\"\\nðŸ”§ Applying KNN Imputation (k=5) to {len(missing_cols)} columns...\")\n",
        "            self.imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
        "            X_imputed = self.imputer.fit_transform(self.X)\n",
        "            self.X = pd.DataFrame(X_imputed, columns=self.X.columns, index=self.X.index)\n",
        "        else:\n",
        "            # Simple median imputation\n",
        "            print(f\"\\nðŸ”§ Applying Median Imputation to {len(missing_cols)} columns...\")\n",
        "            self.imputer = SimpleImputer(strategy='median')\n",
        "            X_imputed = self.imputer.fit_transform(self.X)\n",
        "            self.X = pd.DataFrame(X_imputed, columns=self.X.columns, index=self.X.index)\n",
        "\n",
        "        print(f\"âœ“ Missing values handled successfully\")\n",
        "        print(f\"âœ“ Final feature count: {self.X.shape[1]}\")\n",
        "\n",
        "        return self.X\n",
        "\n",
        "    def create_features(self):\n",
        "        \"\"\"Create advanced engineered features\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"FEATURE ENGINEERING\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        X_new = self.X.copy()\n",
        "\n",
        "        # 1. Polynomial features (selected important features)\n",
        "        if 'st_rad' in X_new.columns and 'st_teff' in X_new.columns:\n",
        "            X_new['luminosity_proxy'] = X_new['st_rad']**2 * X_new['st_teff']**4\n",
        "            print(\"âœ“ Created: luminosity_proxy\")\n",
        "\n",
        "        # 2. Ratio features\n",
        "        if 'pl_rade' in X_new.columns and 'st_rad' in X_new.columns:\n",
        "            X_new['planet_star_radius_ratio'] = X_new['pl_rade'] / (X_new['st_rad'] + 1e-10)\n",
        "            print(\"âœ“ Created: planet_star_radius_ratio\")\n",
        "\n",
        "        # 3. Transit depth proxy\n",
        "        if 'pl_rade' in X_new.columns and 'st_rad' in X_new.columns:\n",
        "            X_new['transit_depth_proxy'] = (X_new['pl_rade'] / (X_new['st_rad'] + 1e-10))**2\n",
        "            print(\"âœ“ Created: transit_depth_proxy\")\n",
        "\n",
        "        # 4. Distance features\n",
        "        if 'st_dist' in X_new.columns:\n",
        "            X_new['log_distance'] = np.log1p(X_new['st_dist'])\n",
        "            X_new['inv_distance'] = 1 / (X_new['st_dist'] + 1e-10)\n",
        "            print(\"âœ“ Created: log_distance, inv_distance\")\n",
        "\n",
        "        # 5. Temperature ratio\n",
        "        if 'pl_eqt' in X_new.columns and 'st_teff' in X_new.columns:\n",
        "            X_new['temp_ratio'] = X_new['pl_eqt'] / (X_new['st_teff'] + 1e-10)\n",
        "            print(\"âœ“ Created: temp_ratio\")\n",
        "\n",
        "        # 6. Insolation features\n",
        "        if 'pl_insol' in X_new.columns:\n",
        "            X_new['log_insol'] = np.log1p(X_new['pl_insol'])\n",
        "            X_new['sqrt_insol'] = np.sqrt(X_new['pl_insol'] + 1e-10)\n",
        "            print(\"âœ“ Created: log_insol, sqrt_insol\")\n",
        "\n",
        "        # 7. Orbital period features\n",
        "        if 'pl_orbper' in X_new.columns:\n",
        "            X_new['log_orbper'] = np.log1p(X_new['pl_orbper'])\n",
        "            print(\"âœ“ Created: log_orbper\")\n",
        "\n",
        "        # 8. Error ratio features (uncertainty indicators)\n",
        "        error_pairs = [\n",
        "            ('st_pmra', 'st_pmraerr1'),\n",
        "            ('st_pmdec', 'st_pmdecerr1'),\n",
        "            ('pl_rade', 'pl_radeerr1'),\n",
        "        ]\n",
        "\n",
        "        for base, error in error_pairs:\n",
        "            if base in X_new.columns and error in X_new.columns:\n",
        "                X_new[f'{base}_error_ratio'] = abs(X_new[error]) / (abs(X_new[base]) + 1e-10)\n",
        "                print(f\"âœ“ Created: {base}_error_ratio\")\n",
        "\n",
        "        # 9. Statistical features\n",
        "        numeric_cols = X_new.select_dtypes(include=[np.number]).columns[:10]  # First 10 numeric cols\n",
        "        if len(numeric_cols) > 3:\n",
        "            X_new['feature_mean'] = X_new[numeric_cols].mean(axis=1)\n",
        "            X_new['feature_std'] = X_new[numeric_cols].std(axis=1)\n",
        "            X_new['feature_max'] = X_new[numeric_cols].max(axis=1)\n",
        "            X_new['feature_min'] = X_new[numeric_cols].min(axis=1)\n",
        "            print(\"âœ“ Created: aggregated statistical features\")\n",
        "\n",
        "        # Replace infinities\n",
        "        X_new = X_new.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "        # Fill any new NaN values created during feature engineering\n",
        "        for col in X_new.columns:\n",
        "            if X_new[col].isnull().any():\n",
        "                X_new[col].fillna(X_new[col].median(), inplace=True)\n",
        "\n",
        "        print(f\"\\nâœ“ Total features after engineering: {X_new.shape[1]}\")\n",
        "\n",
        "        self.X = X_new\n",
        "        return self.X\n",
        "\n",
        "    def remove_outliers(self, contamination=0.05):\n",
        "        \"\"\"Remove outliers using IQR method\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"OUTLIER DETECTION AND REMOVAL\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        original_shape = self.X.shape[0]\n",
        "\n",
        "        # IQR method for outlier detection\n",
        "        Q1 = self.X.quantile(0.25)\n",
        "        Q3 = self.X.quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        # Define outlier boundaries\n",
        "        lower_bound = Q1 - 3 * IQR\n",
        "        upper_bound = Q3 + 3 * IQR\n",
        "\n",
        "        # Identify outliers\n",
        "        outlier_mask = ~((self.X < lower_bound) | (self.X > upper_bound)).any(axis=1)\n",
        "\n",
        "        self.X = self.X[outlier_mask]\n",
        "        self.y = self.y[outlier_mask]\n",
        "\n",
        "        removed = original_shape - self.X.shape[0]\n",
        "        print(f\"\\nâœ“ Removed {removed} outlier samples ({removed/original_shape*100:.2f}%)\")\n",
        "        print(f\"âœ“ Remaining samples: {self.X.shape[0]}\")\n",
        "\n",
        "        return self.X, self.y\n",
        "\n",
        "    def scale_features(self, method='robust'):\n",
        "        \"\"\"Scale features using specified method\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"FEATURE SCALING\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        if method == 'robust':\n",
        "            self.scaler = RobustScaler()\n",
        "            print(\"\\nðŸ”§ Applying Robust Scaling (resistant to outliers)...\")\n",
        "        else:\n",
        "            self.scaler = StandardScaler()\n",
        "            print(\"\\nðŸ”§ Applying Standard Scaling...\")\n",
        "\n",
        "        X_scaled = self.scaler.fit_transform(self.X)\n",
        "        self.X = pd.DataFrame(X_scaled, columns=self.X.columns, index=self.X.index)\n",
        "\n",
        "        print(f\"âœ“ Features scaled: {self.X.shape[1]} features\")\n",
        "\n",
        "        return self.X\n",
        "\n",
        "    def select_features(self, k=60):\n",
        "        \"\"\"Select top k features using mutual information\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"FEATURE SELECTION\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        if self.X.shape[1] <= k:\n",
        "            print(f\"\\nâœ“ Number of features ({self.X.shape[1]}) <= k ({k}), keeping all features\")\n",
        "            self.selected_features = self.X.columns.tolist()\n",
        "            feature_scores = pd.DataFrame({\n",
        "                'feature': self.X.columns,\n",
        "                'score': [1.0] * len(self.X.columns)\n",
        "            })\n",
        "            return self.X, feature_scores\n",
        "\n",
        "        print(f\"\\nðŸ”§ Selecting top {k} features using Mutual Information...\")\n",
        "\n",
        "        selector = SelectKBest(mutual_info_classif, k=k)\n",
        "        X_selected = selector.fit_transform(self.X, self.y)\n",
        "\n",
        "        # Get selected feature names\n",
        "        selected_indices = selector.get_support(indices=True)\n",
        "        self.selected_features = self.X.columns[selected_indices].tolist()\n",
        "\n",
        "        # Get feature scores\n",
        "        feature_scores = pd.DataFrame({\n",
        "            'feature': self.X.columns,\n",
        "            'score': selector.scores_\n",
        "        }).sort_values('score', ascending=False)\n",
        "\n",
        "        print(f\"\\nâœ“ Selected {k} features\")\n",
        "        print(\"\\nðŸ“Š Top 15 Most Important Features:\")\n",
        "        print(feature_scores.head(15).to_string(index=False))\n",
        "\n",
        "        self.X = pd.DataFrame(X_selected, columns=self.selected_features, index=self.X.index)\n",
        "\n",
        "        return self.X, feature_scores\n",
        "\n",
        "\n",
        "class ModelBuilder:\n",
        "    \"\"\"Build, train, and ensemble multiple models\"\"\"\n",
        "\n",
        "    def __init__(self, X_train, X_test, y_train, y_test):\n",
        "        self.X_train = X_train\n",
        "        self.X_test = X_test\n",
        "        self.y_train = y_train\n",
        "        self.y_test = y_test\n",
        "        self.models = {}\n",
        "        self.predictions = {}\n",
        "        self.probabilities = {}\n",
        "        self.ensemble_model = None\n",
        "\n",
        "    def build_models(self):\n",
        "        \"\"\"Build multiple classification models with optimized hyperparameters\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"BUILDING MODELS (OPTIMIZED HYPERPARAMETERS)\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Define models with improved hyperparameters\n",
        "        self.models = {\n",
        "            'LightGBM': lgb.LGBMClassifier(\n",
        "                n_estimators=800,\n",
        "                learning_rate=0.03,\n",
        "                max_depth=10,\n",
        "                num_leaves=50,\n",
        "                min_child_samples=15,\n",
        "                subsample=0.85,\n",
        "                colsample_bytree=0.85,\n",
        "                reg_alpha=0.1,\n",
        "                reg_lambda=0.1,\n",
        "                min_split_gain=0.01,\n",
        "                random_state=RANDOM_STATE,\n",
        "                verbose=-1,\n",
        "                n_jobs=-1,\n",
        "                class_weight='balanced'\n",
        "            ),\n",
        "            'XGBoost': xgb.XGBClassifier(\n",
        "                n_estimators=800,\n",
        "                learning_rate=0.03,\n",
        "                max_depth=9,\n",
        "                min_child_weight=2,\n",
        "                subsample=0.85,\n",
        "                colsample_bytree=0.85,\n",
        "                colsample_bylevel=0.85,\n",
        "                gamma=0.1,\n",
        "                reg_alpha=0.1,\n",
        "                reg_lambda=1.0,\n",
        "                scale_pos_weight=1,\n",
        "                random_state=RANDOM_STATE,\n",
        "                eval_metric='mlogloss',\n",
        "                n_jobs=-1,\n",
        "                tree_method='hist'\n",
        "            ),\n",
        "            'RandomForest': RandomForestClassifier(\n",
        "                n_estimators=700,\n",
        "                max_depth=25,\n",
        "                min_samples_split=4,\n",
        "                min_samples_leaf=1,\n",
        "                max_features='sqrt',\n",
        "                bootstrap=True,\n",
        "                oob_score=True,\n",
        "                class_weight='balanced',\n",
        "                random_state=RANDOM_STATE,\n",
        "                n_jobs=-1\n",
        "            ),\n",
        "            'ExtraTrees': RandomForestClassifier(\n",
        "                n_estimators=700,\n",
        "                max_depth=30,\n",
        "                min_samples_split=3,\n",
        "                min_samples_leaf=1,\n",
        "                max_features='sqrt',\n",
        "                bootstrap=False,\n",
        "                class_weight='balanced',\n",
        "                criterion='gini',\n",
        "                random_state=RANDOM_STATE,\n",
        "                n_jobs=-1\n",
        "            ),\n",
        "            'LogisticRegression': LogisticRegression(\n",
        "                max_iter=2000,\n",
        "                multi_class='multinomial',\n",
        "                solver='saga',\n",
        "                C=0.5,\n",
        "                penalty='l2',\n",
        "                class_weight='balanced',\n",
        "                random_state=RANDOM_STATE,\n",
        "                n_jobs=-1\n",
        "            )\n",
        "        }\n",
        "\n",
        "        print(f\"\\nâœ“ Built {len(self.models)} models with optimized parameters:\")\n",
        "        for name in self.models.keys():\n",
        "            print(f\"  â€¢ {name}\")\n",
        "\n",
        "        return self.models\n",
        "\n",
        "    def train_models(self):\n",
        "        \"\"\"Train all models\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"TRAINING MODELS\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            print(f\"\\nðŸ”§ Training {name}...\")\n",
        "            start_time = datetime.now()\n",
        "\n",
        "            model.fit(self.X_train, self.y_train)\n",
        "\n",
        "            # Make predictions\n",
        "            self.predictions[name] = model.predict(self.X_test)\n",
        "            self.probabilities[name] = model.predict_proba(self.X_test)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            accuracy = accuracy_score(self.y_test, self.predictions[name])\n",
        "\n",
        "            elapsed_time = (datetime.now() - start_time).total_seconds()\n",
        "            print(f\"âœ“ {name} trained in {elapsed_time:.2f}s - Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        return self.predictions\n",
        "\n",
        "    def create_ensemble(self):\n",
        "        \"\"\"Create weighted voting ensemble of best models\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"CREATING ENSEMBLE MODEL\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Use top 4 models for ensemble with optimized weights\n",
        "        ensemble_models = [\n",
        "            ('lgb', self.models['LightGBM']),\n",
        "            ('xgb', self.models['XGBoost']),\n",
        "            ('rf', self.models['RandomForest']),\n",
        "            ('et', self.models['ExtraTrees'])\n",
        "        ]\n",
        "\n",
        "        print(\"\\nðŸ”§ Building Weighted Soft Voting Classifier...\")\n",
        "        # Weights based on expected performance (LightGBM, XGBoost get higher weights)\n",
        "        self.ensemble_model = VotingClassifier(\n",
        "            estimators=ensemble_models,\n",
        "            voting='soft',\n",
        "            weights=[1.5, 1.5, 1.0, 1.0],\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        print(\"ðŸ”§ Training Ensemble...\")\n",
        "        self.ensemble_model.fit(self.X_train, self.y_train)\n",
        "\n",
        "        # Make predictions\n",
        "        self.predictions['Ensemble'] = self.ensemble_model.predict(self.X_test)\n",
        "        self.probabilities['Ensemble'] = self.ensemble_model.predict_proba(self.X_test)\n",
        "\n",
        "        accuracy = accuracy_score(self.y_test, self.predictions['Ensemble'])\n",
        "        print(f\"âœ“ Ensemble trained - Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        return self.ensemble_model\n",
        "\n",
        "    def cross_validate(self, cv_folds=5):\n",
        "        \"\"\"Perform cross-validation on all models\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"CROSS-VALIDATION\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        cv_results = {}\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            print(f\"\\nðŸ”§ Cross-validating {name}...\")\n",
        "\n",
        "            skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n",
        "            scores = cross_val_score(model, self.X_train, self.y_train, cv=skf, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "            cv_results[name] = {\n",
        "                'mean': scores.mean(),\n",
        "                'std': scores.std(),\n",
        "                'scores': scores\n",
        "            }\n",
        "\n",
        "            print(f\"  Mean Accuracy: {scores.mean():.4f} (+/- {scores.std()*2:.4f})\")\n",
        "\n",
        "        return cv_results\n",
        "\n",
        "\n",
        "class ModelEvaluator:\n",
        "    \"\"\"Comprehensive model evaluation and visualization\"\"\"\n",
        "\n",
        "    def __init__(self, models_dict, predictions_dict, probabilities_dict,\n",
        "                 y_test, label_encoder, results_dir):\n",
        "        self.models = models_dict\n",
        "        self.predictions = predictions_dict\n",
        "        self.probabilities = probabilities_dict\n",
        "        self.y_test = y_test\n",
        "        self.label_encoder = label_encoder\n",
        "        self.results_dir = Path(results_dir)\n",
        "        self.results_dir.mkdir(exist_ok=True)\n",
        "        self.metrics_summary = {}\n",
        "\n",
        "    def calculate_metrics(self):\n",
        "        \"\"\"Calculate comprehensive metrics for all models\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"CALCULATING METRICS\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        for name in self.predictions.keys():\n",
        "            y_pred = self.predictions[name]\n",
        "            y_proba = self.probabilities[name]\n",
        "\n",
        "            # Basic metrics\n",
        "            accuracy = accuracy_score(self.y_test, y_pred)\n",
        "            precision, recall, f1, support = precision_recall_fscore_support(\n",
        "                self.y_test, y_pred, average='weighted'\n",
        "            )\n",
        "\n",
        "            # Additional metrics\n",
        "            mcc = matthews_corrcoef(self.y_test, y_pred)\n",
        "            kappa = cohen_kappa_score(self.y_test, y_pred)\n",
        "            logloss = log_loss(self.y_test, y_proba)\n",
        "\n",
        "            # Multi-class AUC (ovr)\n",
        "            try:\n",
        "                auc_ovr = roc_auc_score(self.y_test, y_proba, multi_class='ovr', average='weighted')\n",
        "            except:\n",
        "                auc_ovr = np.nan\n",
        "\n",
        "            self.metrics_summary[name] = {\n",
        "                'Accuracy': accuracy,\n",
        "                'Precision': precision,\n",
        "                'Recall': recall,\n",
        "                'F1-Score': f1,\n",
        "                'MCC': mcc,\n",
        "                'Cohen_Kappa': kappa,\n",
        "                'Log_Loss': logloss,\n",
        "                'AUC_OVR': auc_ovr\n",
        "            }\n",
        "\n",
        "            print(f\"\\nðŸ“Š {name} Metrics:\")\n",
        "            print(f\"  Accuracy:     {accuracy:.4f}\")\n",
        "            print(f\"  Precision:    {precision:.4f}\")\n",
        "            print(f\"  Recall:       {recall:.4f}\")\n",
        "            print(f\"  F1-Score:     {f1:.4f}\")\n",
        "            print(f\"  MCC:          {mcc:.4f}\")\n",
        "            print(f\"  Cohen Kappa:  {kappa:.4f}\")\n",
        "            print(f\"  Log Loss:     {logloss:.4f}\")\n",
        "            print(f\"  AUC (OVR):    {auc_ovr:.4f}\")\n",
        "\n",
        "        # Save metrics to JSON\n",
        "        metrics_file = self.results_dir / 'metrics_summary.json'\n",
        "        with open(metrics_file, 'w') as f:\n",
        "            json.dump(self.metrics_summary, f, indent=4)\n",
        "        print(f\"\\nâœ“ Metrics saved to {metrics_file}\")\n",
        "\n",
        "        return self.metrics_summary\n",
        "\n",
        "    def plot_model_comparison(self):\n",
        "        \"\"\"Create comprehensive model comparison plots\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"CREATING MODEL COMPARISON PLOTS\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Extract metrics for plotting\n",
        "        models_list = list(self.metrics_summary.keys())\n",
        "        metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'MCC', 'Cohen_Kappa']\n",
        "\n",
        "        # Create subplots\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "        fig.suptitle('Model Performance Comparison', fontsize=20, fontweight='bold')\n",
        "        axes = axes.ravel()\n",
        "\n",
        "        for idx, metric in enumerate(metrics_names):\n",
        "            values = [self.metrics_summary[model][metric] for model in models_list]\n",
        "\n",
        "            # Create bar plot\n",
        "            bars = axes[idx].bar(models_list, values, color=plt.cm.viridis(np.linspace(0, 1, len(models_list))))\n",
        "            axes[idx].set_title(f'{metric}', fontsize=14, fontweight='bold')\n",
        "            axes[idx].set_ylabel('Score', fontsize=12)\n",
        "            axes[idx].set_ylim([0, 1])\n",
        "            axes[idx].grid(axis='y', alpha=0.3)\n",
        "            axes[idx].tick_params(axis='x', rotation=45)\n",
        "\n",
        "            # Add value labels on bars\n",
        "            for bar in bars:\n",
        "                height = bar.get_height()\n",
        "                axes[idx].text(bar.get_x() + bar.get_width()/2., height,\n",
        "                             f'{height:.3f}',\n",
        "                             ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plot_file = self.results_dir / 'model_comparison.png'\n",
        "        plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"âœ“ Model comparison plot saved to {plot_file}\")\n",
        "\n",
        "    def plot_confusion_matrices(self):\n",
        "        \"\"\"Plot confusion matrices for all models\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"CREATING CONFUSION MATRICES\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        n_models = len(self.predictions)\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(20, 14))\n",
        "        fig.suptitle('Confusion Matrices', fontsize=20, fontweight='bold')\n",
        "        axes = axes.ravel()\n",
        "\n",
        "        for idx, (name, y_pred) in enumerate(self.predictions.items()):\n",
        "            if idx >= len(axes):\n",
        "                break\n",
        "\n",
        "            cm = confusion_matrix(self.y_test, y_pred)\n",
        "\n",
        "            # Normalize confusion matrix\n",
        "            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "            # Plot\n",
        "            sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
        "                       ax=axes[idx], cbar_kws={'label': 'Proportion'},\n",
        "                       xticklabels=self.label_encoder.classes_,\n",
        "                       yticklabels=self.label_encoder.classes_)\n",
        "            axes[idx].set_title(f'{name}\\nAccuracy: {self.metrics_summary[name][\"Accuracy\"]:.4f}',\n",
        "                              fontsize=12, fontweight='bold')\n",
        "            axes[idx].set_ylabel('True Label', fontsize=11)\n",
        "            axes[idx].set_xlabel('Predicted Label', fontsize=11)\n",
        "\n",
        "        # Hide unused subplots\n",
        "        for idx in range(len(self.predictions), len(axes)):\n",
        "            axes[idx].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plot_file = self.results_dir / 'confusion_matrices.png'\n",
        "        plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"âœ“ Confusion matrices saved to {plot_file}\")\n",
        "\n",
        "    def plot_classification_reports(self):\n",
        "        \"\"\"Create detailed classification reports\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"CREATING CLASSIFICATION REPORTS\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        for name, y_pred in self.predictions.items():\n",
        "            print(f\"\\nðŸ“Š {name} Classification Report:\")\n",
        "            report = classification_report(\n",
        "                self.y_test, y_pred,\n",
        "                target_names=self.label_encoder.classes_,\n",
        "                digits=4\n",
        "            )\n",
        "            print(report)\n",
        "\n",
        "            # Save to file\n",
        "            report_file = self.results_dir / f'classification_report_{name}.txt'\n",
        "            with open(report_file, 'w') as f:\n",
        "                f.write(f\"Classification Report: {name}\\n\")\n",
        "                f.write(\"=\"*70 + \"\\n\\n\")\n",
        "                f.write(report)\n",
        "\n",
        "            print(f\"âœ“ Classification report saved to {report_file}\")\n",
        "\n",
        "    def plot_feature_importance(self, feature_names, top_n=20):\n",
        "        \"\"\"Plot feature importance for tree-based models\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"CREATING FEATURE IMPORTANCE PLOTS\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        importance_models = ['LightGBM', 'XGBoost', 'RandomForest', 'ExtraTrees']\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
        "        fig.suptitle('Feature Importance Analysis', fontsize=20, fontweight='bold')\n",
        "        axes = axes.ravel()\n",
        "\n",
        "        for idx, name in enumerate(importance_models):\n",
        "            if name not in self.models:\n",
        "                continue\n",
        "\n",
        "            model = self.models[name]\n",
        "\n",
        "            # Get feature importance\n",
        "            if hasattr(model, 'feature_importances_'):\n",
        "                importances = model.feature_importances_\n",
        "\n",
        "                # Create dataframe\n",
        "                importance_df = pd.DataFrame({\n",
        "                    'feature': feature_names,\n",
        "                    'importance': importances\n",
        "                }).sort_values('importance', ascending=False).head(top_n)\n",
        "\n",
        "                # Plot\n",
        "                axes[idx].barh(range(len(importance_df)), importance_df['importance'],\n",
        "                             color=plt.cm.plasma(np.linspace(0, 1, len(importance_df))))\n",
        "                axes[idx].set_yticks(range(len(importance_df)))\n",
        "                axes[idx].set_yticklabels(importance_df['feature'], fontsize=9)\n",
        "                axes[idx].set_xlabel('Importance', fontsize=11)\n",
        "                axes[idx].set_title(f'{name} - Top {top_n} Features', fontsize=13, fontweight='bold')\n",
        "                axes[idx].invert_yaxis()\n",
        "                axes[idx].grid(axis='x', alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plot_file = self.results_dir / 'feature_importance.png'\n",
        "        plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"âœ“ Feature importance plot saved to {plot_file}\")\n",
        "\n",
        "    def plot_learning_curves(self, X_train, y_train):\n",
        "        \"\"\"Plot learning curves to analyze model performance\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"CREATING LEARNING CURVES\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        from sklearn.model_selection import learning_curve\n",
        "\n",
        "        # Select a few models for learning curves\n",
        "        models_to_plot = ['LightGBM', 'XGBoost', 'RandomForest']\n",
        "\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "        fig.suptitle('Learning Curves', fontsize=20, fontweight='bold')\n",
        "\n",
        "        for idx, name in enumerate(models_to_plot):\n",
        "            if name not in self.models:\n",
        "                continue\n",
        "\n",
        "            print(f\"ðŸ”§ Computing learning curve for {name}...\")\n",
        "\n",
        "            model = self.models[name]\n",
        "            train_sizes, train_scores, val_scores = learning_curve(\n",
        "                model, X_train, y_train, cv=3,\n",
        "                train_sizes=np.linspace(0.1, 1.0, 10),\n",
        "                scoring='accuracy', n_jobs=-1, random_state=RANDOM_STATE\n",
        "            )\n",
        "\n",
        "            # Calculate means and stds\n",
        "            train_mean = np.mean(train_scores, axis=1)\n",
        "            train_std = np.std(train_scores, axis=1)\n",
        "            val_mean = np.mean(val_scores, axis=1)\n",
        "            val_std = np.std(val_scores, axis=1)\n",
        "\n",
        "            # Plot\n",
        "            axes[idx].plot(train_sizes, train_mean, label='Training score',\n",
        "                          color='blue', marker='o', linewidth=2)\n",
        "            axes[idx].fill_between(train_sizes, train_mean - train_std,\n",
        "                                  train_mean + train_std, alpha=0.15, color='blue')\n",
        "\n",
        "            axes[idx].plot(train_sizes, val_mean, label='Validation score',\n",
        "                          color='red', marker='s', linewidth=2)\n",
        "            axes[idx].fill_between(train_sizes, val_mean - val_std,\n",
        "                                  val_mean + val_std, alpha=0.15, color='red')\n",
        "\n",
        "            axes[idx].set_xlabel('Training Set Size', fontsize=11)\n",
        "            axes[idx].set_ylabel('Accuracy Score', fontsize=11)\n",
        "            axes[idx].set_title(f'{name}', fontsize=13, fontweight='bold')\n",
        "            axes[idx].legend(loc='best', fontsize=10)\n",
        "            axes[idx].grid(alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plot_file = self.results_dir / 'learning_curves.png'\n",
        "        plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"âœ“ Learning curves saved to {plot_file}\")\n",
        "\n",
        "    def plot_roc_curves(self):\n",
        "        \"\"\"Plot ROC curves for multiclass classification (One-vs-Rest)\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"CREATING ROC CURVES\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        from sklearn.preprocessing import label_binarize\n",
        "\n",
        "        n_classes = len(self.label_encoder.classes_)\n",
        "        y_test_bin = label_binarize(self.y_test, classes=range(n_classes))\n",
        "\n",
        "        # Plot for best model (Ensemble)\n",
        "        if 'Ensemble' in self.probabilities:\n",
        "            fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "            y_proba = self.probabilities['Ensemble']\n",
        "\n",
        "            # Compute ROC curve and AUC for each class\n",
        "            for i in range(n_classes):\n",
        "                fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_proba[:, i])\n",
        "                auc_score = roc_auc_score(y_test_bin[:, i], y_proba[:, i])\n",
        "\n",
        "                ax.plot(fpr, tpr, linewidth=2,\n",
        "                       label=f'{self.label_encoder.classes_[i]} (AUC = {auc_score:.3f})')\n",
        "\n",
        "            ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n",
        "            ax.set_xlabel('False Positive Rate', fontsize=13)\n",
        "            ax.set_ylabel('True Positive Rate', fontsize=13)\n",
        "            ax.set_title('ROC Curves - Ensemble Model (One-vs-Rest)',\n",
        "                        fontsize=15, fontweight='bold')\n",
        "            ax.legend(loc='lower right', fontsize=10)\n",
        "            ax.grid(alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plot_file = self.results_dir / 'roc_curves_ensemble.png'\n",
        "            plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            print(f\"âœ“ ROC curves saved to {plot_file}\")\n",
        "\n",
        "    def plot_prediction_distribution(self):\n",
        "        \"\"\"Plot distribution of predictions vs actual\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"CREATING PREDICTION DISTRIBUTION PLOTS\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "        fig.suptitle('Prediction Distribution Analysis', fontsize=20, fontweight='bold')\n",
        "        axes = axes.ravel()\n",
        "\n",
        "        for idx, (name, y_pred) in enumerate(self.predictions.items()):\n",
        "            if idx >= len(axes):\n",
        "                break\n",
        "\n",
        "            # Count predictions and actuals\n",
        "            pred_counts = pd.Series(y_pred).value_counts().sort_index()\n",
        "            actual_counts = pd.Series(self.y_test).value_counts().sort_index()\n",
        "\n",
        "            # Create comparison dataframe\n",
        "            comparison_df = pd.DataFrame({\n",
        "                'Actual': actual_counts,\n",
        "                'Predicted': pred_counts\n",
        "            }).fillna(0)\n",
        "\n",
        "            # Plot\n",
        "            x = np.arange(len(comparison_df))\n",
        "            width = 0.35\n",
        "\n",
        "            axes[idx].bar(x - width/2, comparison_df['Actual'], width,\n",
        "                         label='Actual', alpha=0.8, color='steelblue')\n",
        "            axes[idx].bar(x + width/2, comparison_df['Predicted'], width,\n",
        "                         label='Predicted', alpha=0.8, color='coral')\n",
        "\n",
        "            axes[idx].set_xlabel('Class', fontsize=11)\n",
        "            axes[idx].set_ylabel('Count', fontsize=11)\n",
        "            axes[idx].set_title(f'{name}', fontsize=13, fontweight='bold')\n",
        "            axes[idx].set_xticks(x)\n",
        "            axes[idx].set_xticklabels(self.label_encoder.classes_)\n",
        "            axes[idx].legend(fontsize=10)\n",
        "            axes[idx].grid(axis='y', alpha=0.3)\n",
        "\n",
        "        # Hide unused subplots\n",
        "        for idx in range(len(self.predictions), len(axes)):\n",
        "            axes[idx].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plot_file = self.results_dir / 'prediction_distribution.png'\n",
        "        plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"âœ“ Prediction distribution plot saved to {plot_file}\")\n",
        "\n",
        "\n",
        "def save_artifacts(results_dir, models, scaler, imputer, label_encoder,\n",
        "                  feature_names, selected_features, feature_scores):\n",
        "    \"\"\"Save all models and preprocessing artifacts\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"SAVING MODELS AND ARTIFACTS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    results_path = Path(results_dir)\n",
        "\n",
        "    # Save models\n",
        "    for name, model in models.items():\n",
        "        model_file = results_path / f'model_{name.lower().replace(\" \", \"_\")}.pkl'\n",
        "        joblib.dump(model, model_file)\n",
        "        print(f\"âœ“ Saved {name} model to {model_file}\")\n",
        "\n",
        "    # Save preprocessing objects\n",
        "    joblib.dump(scaler, results_path / 'scaler.pkl')\n",
        "    print(f\"âœ“ Saved scaler to {results_path / 'scaler.pkl'}\")\n",
        "\n",
        "    joblib.dump(imputer, results_path / 'imputer.pkl')\n",
        "    print(f\"âœ“ Saved imputer to {results_path / 'imputer.pkl'}\")\n",
        "\n",
        "    joblib.dump(label_encoder, results_path / 'label_encoder.pkl')\n",
        "    print(f\"âœ“ Saved label encoder to {results_path / 'label_encoder.pkl'}\")\n",
        "\n",
        "    # Save feature information\n",
        "    feature_info = {\n",
        "        'all_features': feature_names,\n",
        "        'selected_features': selected_features,\n",
        "        'n_features': len(feature_names),\n",
        "        'n_selected': len(selected_features)\n",
        "    }\n",
        "\n",
        "    with open(results_path / 'feature_info.json', 'w') as f:\n",
        "        json.dump(feature_info, f, indent=4)\n",
        "    print(f\"âœ“ Saved feature info to {results_path / 'feature_info.json'}\")\n",
        "\n",
        "    # Save feature scores\n",
        "    feature_scores.to_csv(results_path / 'feature_scores.csv', index=False)\n",
        "    print(f\"âœ“ Saved feature scores to {results_path / 'feature_scores.csv'}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution pipeline\"\"\"\n",
        "    print(\"\\n\")\n",
        "    print(\"=\"*70)\n",
        "    print(\" \" * 10 + \"MULTICLASS CLASSIFICATION PIPELINE\")\n",
        "    print(\" \" * 15 + \"Target: tfopwg_disp\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Configuration\n",
        "    DATA_FILE = 'TOI_2025.10.03_22.38.38.csv'\n",
        "    RESULTS_DIR = 'results'\n",
        "\n",
        "    # Create results directory\n",
        "    Path(RESULTS_DIR).mkdir(exist_ok=True)\n",
        "\n",
        "    # ========================\n",
        "    # 1. DATA LOADING\n",
        "    # ========================\n",
        "    processor = DataProcessor(DATA_FILE)\n",
        "    df = processor.load_data()\n",
        "    missing_df, numeric_cols, categorical_cols = processor.analyze_data_quality()\n",
        "    X, y, feature_cols = processor.prepare_features_target()\n",
        "\n",
        "    # ========================\n",
        "    # 2. FEATURE ENGINEERING\n",
        "    # ========================\n",
        "    engineer = FeatureEngineer(X, y)\n",
        "    X = engineer.handle_missing_values(strategy='advanced')\n",
        "    X = engineer.create_features()\n",
        "    X, y = engineer.remove_outliers(contamination=0.05)\n",
        "    X = engineer.scale_features(method='robust')\n",
        "    X, feature_scores = engineer.select_features(k=60)\n",
        "\n",
        "    # ========================\n",
        "    # 3. TRAIN-TEST SPLIT\n",
        "    # ========================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"TRAIN-TEST SPLIT\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"\\nâœ“ Training set: {X_train.shape[0]} samples\")\n",
        "    print(f\"âœ“ Test set: {X_test.shape[0]} samples\")\n",
        "    print(f\"âœ“ Feature dimensions: {X_train.shape[1]}\")\n",
        "\n",
        "    # ========================\n",
        "    # 4. MODEL BUILDING & TRAINING\n",
        "    # ========================\n",
        "    builder = ModelBuilder(X_train, X_test, y_train, y_test)\n",
        "    models = builder.build_models()\n",
        "    predictions = builder.train_models()\n",
        "    ensemble_model = builder.create_ensemble()\n",
        "    cv_results = builder.cross_validate(cv_folds=5)\n",
        "\n",
        "    # ========================\n",
        "    # 5. MODEL EVALUATION\n",
        "    # ========================\n",
        "    evaluator = ModelEvaluator(\n",
        "        models_dict=builder.models,\n",
        "        predictions_dict=builder.predictions,\n",
        "        probabilities_dict=builder.probabilities,\n",
        "        y_test=y_test,\n",
        "        label_encoder=processor.label_encoder,\n",
        "        results_dir=RESULTS_DIR\n",
        "    )\n",
        "\n",
        "    metrics = evaluator.calculate_metrics()\n",
        "    evaluator.plot_model_comparison()\n",
        "    evaluator.plot_confusion_matrices()\n",
        "    evaluator.plot_classification_reports()\n",
        "    evaluator.plot_feature_importance(feature_names=engineer.selected_features, top_n=20)\n",
        "    evaluator.plot_learning_curves(X_train, y_train)\n",
        "    evaluator.plot_roc_curves()\n",
        "    evaluator.plot_prediction_distribution()\n",
        "\n",
        "    # ========================\n",
        "    # 6. SAVE ARTIFACTS\n",
        "    # ========================\n",
        "    save_artifacts(\n",
        "        results_dir=RESULTS_DIR,\n",
        "        models=builder.models,\n",
        "        scaler=engineer.scaler,\n",
        "        imputer=engineer.imputer,\n",
        "        label_encoder=processor.label_encoder,\n",
        "        feature_names=feature_cols,\n",
        "        selected_features=engineer.selected_features,\n",
        "        feature_scores=feature_scores\n",
        "    )\n",
        "\n",
        "    # ========================\n",
        "    # 7. FINAL SUMMARY\n",
        "    # ========================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\nðŸ“Š FINAL MODEL RANKINGS:\")\n",
        "    rankings = sorted(metrics.items(), key=lambda x: x[1]['Accuracy'], reverse=True)\n",
        "    for rank, (name, scores) in enumerate(rankings, 1):\n",
        "        print(f\"{rank}. {name:20s} - Accuracy: {scores['Accuracy']:.4f}, F1: {scores['F1-Score']:.4f}\")\n",
        "\n",
        "    print(f\"\\nâœ“ All results saved to '{RESULTS_DIR}/' directory\")\n",
        "    print(f\"âœ“ Models, scalers, and artifacts ready for production deployment\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0kVkb_zDAND",
        "outputId": "3bdadf9a-ca34-42ca-8d56-683fbc39f236"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======================================================================\n",
            "          MULTICLASS CLASSIFICATION PIPELINE\n",
            "               Target: tfopwg_disp\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "LOADING DATA\n",
            "======================================================================\n",
            "\n",
            "âœ“ Data loaded successfully: 7703 rows, 65 columns\n",
            "\n",
            "ðŸ“Š Target Variable Distribution (tfopwg_disp):\n",
            "tfopwg_disp\n",
            "PC     4679\n",
            "FP     1197\n",
            "CP      684\n",
            "KP      583\n",
            "APC     462\n",
            "FA       98\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Target Class Proportions:\n",
            "tfopwg_disp\n",
            "PC     0.6074\n",
            "FP     0.1554\n",
            "CP     0.0888\n",
            "KP     0.0757\n",
            "APC    0.0600\n",
            "FA     0.0127\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "======================================================================\n",
            "DATA QUALITY ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "ðŸ“‹ Columns with Missing Values:\n",
            "              Missing_Count  Missing_Percentage\n",
            "pl_insolerr2           7703              100.00\n",
            "pl_eqterr1             7703              100.00\n",
            "pl_insollim            7703              100.00\n",
            "pl_eqterr2             7703              100.00\n",
            "pl_eqtlim              7703              100.00\n",
            "pl_insolerr1           7703              100.00\n",
            "st_loggerr1            2271               29.48\n",
            "st_loggerr2            2271               29.48\n",
            "st_raderr1             1963               25.48\n",
            "st_raderr2             1963               25.48\n",
            "pl_radeerr1            1623               21.07\n",
            "pl_radeerr2            1623               21.07\n",
            "st_logg                 856               11.11\n",
            "st_disterr1             707                9.18\n",
            "st_disterr2             707                9.18\n",
            "st_rad                  507                6.58\n",
            "pl_rade                 506                6.57\n",
            "st_tefferr2             474                6.15\n",
            "st_tefferr1             474                6.15\n",
            "pl_eqt                  311                4.04\n",
            "\n",
            "ðŸ“ Data Types:\n",
            "float64    49\n",
            "int64      11\n",
            "object      5\n",
            "Name: count, dtype: int64\n",
            "\n",
            "âœ“ Numeric columns: 60\n",
            "âœ“ Categorical columns: 5\n",
            "\n",
            "======================================================================\n",
            "PREPARING FEATURES AND TARGET\n",
            "======================================================================\n",
            "\n",
            "âœ“ Features shape: (7703, 58)\n",
            "âœ“ Target shape: (7703,)\n",
            "âœ“ Number of classes: 6\n",
            "âœ“ Classes: ['APC', 'CP', 'FA', 'FP', 'KP', 'PC']\n",
            "\n",
            "======================================================================\n",
            "HANDLING MISSING VALUES\n",
            "======================================================================\n",
            "\n",
            "ðŸ—‘ï¸  Dropping 6 columns with >95% missing values:\n",
            "  â€¢ pl_insolerr1 (100.0% missing)\n",
            "  â€¢ pl_insolerr2 (100.0% missing)\n",
            "  â€¢ pl_insollim (100.0% missing)\n",
            "  â€¢ pl_eqterr1 (100.0% missing)\n",
            "  â€¢ pl_eqterr2 (100.0% missing)\n",
            "  â€¢ pl_eqtlim (100.0% missing)\n",
            "\n",
            "ðŸ”§ Applying KNN Imputation (k=5) to 34 columns...\n",
            "âœ“ Missing values handled successfully\n",
            "âœ“ Final feature count: 52\n",
            "\n",
            "======================================================================\n",
            "FEATURE ENGINEERING\n",
            "======================================================================\n",
            "âœ“ Created: luminosity_proxy\n",
            "âœ“ Created: planet_star_radius_ratio\n",
            "âœ“ Created: transit_depth_proxy\n",
            "âœ“ Created: log_distance, inv_distance\n",
            "âœ“ Created: temp_ratio\n",
            "âœ“ Created: log_insol, sqrt_insol\n",
            "âœ“ Created: log_orbper\n",
            "âœ“ Created: st_pmra_error_ratio\n",
            "âœ“ Created: st_pmdec_error_ratio\n",
            "âœ“ Created: pl_rade_error_ratio\n",
            "âœ“ Created: aggregated statistical features\n",
            "\n",
            "âœ“ Total features after engineering: 68\n",
            "\n",
            "======================================================================\n",
            "OUTLIER DETECTION AND REMOVAL\n",
            "======================================================================\n",
            "\n",
            "âœ“ Removed 5262 outlier samples (68.31%)\n",
            "âœ“ Remaining samples: 2441\n",
            "\n",
            "======================================================================\n",
            "FEATURE SCALING\n",
            "======================================================================\n",
            "\n",
            "ðŸ”§ Applying Robust Scaling (resistant to outliers)...\n",
            "âœ“ Features scaled: 68 features\n",
            "\n",
            "======================================================================\n",
            "FEATURE SELECTION\n",
            "======================================================================\n",
            "\n",
            "ðŸ”§ Selecting top 60 features using Mutual Information...\n",
            "\n",
            "âœ“ Selected 60 features\n",
            "\n",
            "ðŸ“Š Top 15 Most Important Features:\n",
            "            feature    score\n",
            "         pl_tranmid 0.100584\n",
            "    pl_trandurherr2 0.094336\n",
            "    pl_trandurherr1 0.092609\n",
            "            st_tmag 0.077312\n",
            "pl_rade_error_ratio 0.074635\n",
            "     pl_tranmiderr1 0.070727\n",
            "     pl_tranmiderr2 0.069395\n",
            "     pl_trandeperr2 0.067087\n",
            "     pl_trandeperr1 0.067086\n",
            "             st_rad 0.065305\n",
            "        st_loggerr1 0.060290\n",
            "         log_orbper 0.057101\n",
            "          pl_orbper 0.056731\n",
            "        st_loggerr2 0.055891\n",
            "        st_tefferr1 0.054498\n",
            "\n",
            "======================================================================\n",
            "TRAIN-TEST SPLIT\n",
            "======================================================================\n",
            "\n",
            "âœ“ Training set: 1952 samples\n",
            "âœ“ Test set: 489 samples\n",
            "âœ“ Feature dimensions: 60\n",
            "\n",
            "======================================================================\n",
            "BUILDING MODELS (OPTIMIZED HYPERPARAMETERS)\n",
            "======================================================================\n",
            "\n",
            "âœ“ Built 5 models with optimized parameters:\n",
            "  â€¢ LightGBM\n",
            "  â€¢ XGBoost\n",
            "  â€¢ RandomForest\n",
            "  â€¢ ExtraTrees\n",
            "  â€¢ LogisticRegression\n",
            "\n",
            "======================================================================\n",
            "TRAINING MODELS\n",
            "======================================================================\n",
            "\n",
            "ðŸ”§ Training LightGBM...\n",
            "âœ“ LightGBM trained in 10.47s - Accuracy: 0.7157\n",
            "\n",
            "ðŸ”§ Training XGBoost...\n",
            "âœ“ XGBoost trained in 21.33s - Accuracy: 0.7219\n",
            "\n",
            "ðŸ”§ Training RandomForest...\n",
            "âœ“ RandomForest trained in 10.62s - Accuracy: 0.7035\n",
            "\n",
            "ðŸ”§ Training ExtraTrees...\n",
            "âœ“ ExtraTrees trained in 14.69s - Accuracy: 0.7076\n",
            "\n",
            "ðŸ”§ Training LogisticRegression...\n",
            "âœ“ LogisticRegression trained in 9.37s - Accuracy: 0.5337\n",
            "\n",
            "======================================================================\n",
            "CREATING ENSEMBLE MODEL\n",
            "======================================================================\n",
            "\n",
            "ðŸ”§ Building Weighted Soft Voting Classifier...\n",
            "ðŸ”§ Training Ensemble...\n",
            "âœ“ Ensemble trained - Accuracy: 0.7198\n",
            "\n",
            "======================================================================\n",
            "CROSS-VALIDATION\n",
            "======================================================================\n",
            "\n",
            "ðŸ”§ Cross-validating LightGBM...\n",
            "  Mean Accuracy: 0.7244 (+/- 0.0310)\n",
            "\n",
            "ðŸ”§ Cross-validating XGBoost...\n",
            "  Mean Accuracy: 0.7331 (+/- 0.0164)\n",
            "\n",
            "ðŸ”§ Cross-validating RandomForest...\n",
            "  Mean Accuracy: 0.7187 (+/- 0.0025)\n",
            "\n",
            "ðŸ”§ Cross-validating ExtraTrees...\n",
            "  Mean Accuracy: 0.7193 (+/- 0.0142)\n",
            "\n",
            "ðŸ”§ Cross-validating LogisticRegression...\n",
            "  Mean Accuracy: 0.5051 (+/- 0.0359)\n",
            "\n",
            "======================================================================\n",
            "CALCULATING METRICS\n",
            "======================================================================\n",
            "\n",
            "ðŸ“Š LightGBM Metrics:\n",
            "  Accuracy:     0.7157\n",
            "  Precision:    0.6878\n",
            "  Recall:       0.7157\n",
            "  F1-Score:     0.6977\n",
            "  MCC:          0.4009\n",
            "  Cohen Kappa:  0.3966\n",
            "  Log Loss:     0.8668\n",
            "  AUC (OVR):    0.8349\n",
            "\n",
            "ðŸ“Š XGBoost Metrics:\n",
            "  Accuracy:     0.7219\n",
            "  Precision:    0.6784\n",
            "  Recall:       0.7219\n",
            "  F1-Score:     0.6878\n",
            "  MCC:          0.3763\n",
            "  Cohen Kappa:  0.3590\n",
            "  Log Loss:     0.8242\n",
            "  AUC (OVR):    0.8421\n",
            "\n",
            "ðŸ“Š RandomForest Metrics:\n",
            "  Accuracy:     0.7035\n",
            "  Precision:    0.6087\n",
            "  Recall:       0.7035\n",
            "  F1-Score:     0.6275\n",
            "  MCC:          0.2607\n",
            "  Cohen Kappa:  0.2031\n",
            "  Log Loss:     0.7922\n",
            "  AUC (OVR):    0.8300\n",
            "\n",
            "ðŸ“Š ExtraTrees Metrics:\n",
            "  Accuracy:     0.7076\n",
            "  Precision:    0.6453\n",
            "  Recall:       0.7076\n",
            "  F1-Score:     0.6386\n",
            "  MCC:          0.2798\n",
            "  Cohen Kappa:  0.2245\n",
            "  Log Loss:     0.7801\n",
            "  AUC (OVR):    0.8283\n",
            "\n",
            "ðŸ“Š LogisticRegression Metrics:\n",
            "  Accuracy:     0.5337\n",
            "  Precision:    0.7310\n",
            "  Recall:       0.5337\n",
            "  F1-Score:     0.5809\n",
            "  MCC:          0.3611\n",
            "  Cohen Kappa:  0.3251\n",
            "  Log Loss:     1.2703\n",
            "  AUC (OVR):    0.7944\n",
            "\n",
            "ðŸ“Š Ensemble Metrics:\n",
            "  Accuracy:     0.7198\n",
            "  Precision:    0.6746\n",
            "  Recall:       0.7198\n",
            "  F1-Score:     0.6840\n",
            "  MCC:          0.3675\n",
            "  Cohen Kappa:  0.3487\n",
            "  Log Loss:     0.7484\n",
            "  AUC (OVR):    0.8429\n",
            "\n",
            "âœ“ Metrics saved to results/metrics_summary.json\n",
            "\n",
            "======================================================================\n",
            "CREATING MODEL COMPARISON PLOTS\n",
            "======================================================================\n",
            "âœ“ Model comparison plot saved to results/model_comparison.png\n",
            "\n",
            "======================================================================\n",
            "CREATING CONFUSION MATRICES\n",
            "======================================================================\n",
            "âœ“ Confusion matrices saved to results/confusion_matrices.png\n",
            "\n",
            "======================================================================\n",
            "CREATING CLASSIFICATION REPORTS\n",
            "======================================================================\n",
            "\n",
            "ðŸ“Š LightGBM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         APC     0.4167    0.2632    0.3226        19\n",
            "          CP     0.3214    0.2727    0.2951        33\n",
            "          FA     0.0000    0.0000    0.0000         1\n",
            "          FP     0.4000    0.2400    0.3000        50\n",
            "          KP     0.6000    0.6111    0.6055        54\n",
            "          PC     0.7995    0.8765    0.8362       332\n",
            "\n",
            "    accuracy                         0.7157       489\n",
            "   macro avg     0.4229    0.3773    0.3932       489\n",
            "weighted avg     0.6878    0.7157    0.6977       489\n",
            "\n",
            "âœ“ Classification report saved to results/classification_report_LightGBM.txt\n",
            "\n",
            "ðŸ“Š XGBoost Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         APC     0.5000    0.2105    0.2963        19\n",
            "          CP     0.3478    0.2424    0.2857        33\n",
            "          FA     0.0000    0.0000    0.0000         1\n",
            "          FP     0.3333    0.1400    0.1972        50\n",
            "          KP     0.7143    0.5556    0.6250        54\n",
            "          PC     0.7696    0.9157    0.8363       332\n",
            "\n",
            "    accuracy                         0.7219       489\n",
            "   macro avg     0.4442    0.3440    0.3734       489\n",
            "weighted avg     0.6784    0.7219    0.6878       489\n",
            "\n",
            "âœ“ Classification report saved to results/classification_report_XGBoost.txt\n",
            "\n",
            "ðŸ“Š RandomForest Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         APC     0.0000    0.0000    0.0000        19\n",
            "          CP     0.2222    0.0606    0.0952        33\n",
            "          FA     0.0000    0.0000    0.0000         1\n",
            "          FP     0.1429    0.0200    0.0351        50\n",
            "          KP     0.8462    0.4074    0.5500        54\n",
            "          PC     0.7152    0.9608    0.8201       332\n",
            "\n",
            "    accuracy                         0.7035       489\n",
            "   macro avg     0.3211    0.2415    0.2501       489\n",
            "weighted avg     0.6087    0.7035    0.6275       489\n",
            "\n",
            "âœ“ Classification report saved to results/classification_report_RandomForest.txt\n",
            "\n",
            "ðŸ“Š ExtraTrees Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         APC     0.5000    0.0526    0.0952        19\n",
            "          CP     0.2727    0.0909    0.1364        33\n",
            "          FA     0.0000    0.0000    0.0000         1\n",
            "          FP     0.2500    0.0400    0.0690        50\n",
            "          KP     0.8462    0.4074    0.5500        54\n",
            "          PC     0.7195    0.9578    0.8217       332\n",
            "\n",
            "    accuracy                         0.7076       489\n",
            "   macro avg     0.4314    0.2581    0.2787       489\n",
            "weighted avg     0.6453    0.7076    0.6386       489\n",
            "\n",
            "âœ“ Classification report saved to results/classification_report_ExtraTrees.txt\n",
            "\n",
            "ðŸ“Š LogisticRegression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         APC     0.1739    0.6316    0.2727        19\n",
            "          CP     0.2278    0.5455    0.3214        33\n",
            "          FA     0.0000    0.0000    0.0000         1\n",
            "          FP     0.4333    0.5200    0.4727        50\n",
            "          KP     0.3626    0.6111    0.4552        54\n",
            "          PC     0.9198    0.5181    0.6628       332\n",
            "\n",
            "    accuracy                         0.5337       489\n",
            "   macro avg     0.3529    0.4710    0.3641       489\n",
            "weighted avg     0.7310    0.5337    0.5809       489\n",
            "\n",
            "âœ“ Classification report saved to results/classification_report_LogisticRegression.txt\n",
            "\n",
            "ðŸ“Š Ensemble Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         APC     0.5000    0.2105    0.2963        19\n",
            "          CP     0.3500    0.2121    0.2642        33\n",
            "          FA     0.0000    0.0000    0.0000         1\n",
            "          FP     0.3333    0.1400    0.1972        50\n",
            "          KP     0.7143    0.5556    0.6250        54\n",
            "          PC     0.7638    0.9157    0.8329       332\n",
            "\n",
            "    accuracy                         0.7198       489\n",
            "   macro avg     0.4436    0.3390    0.3693       489\n",
            "weighted avg     0.6746    0.7198    0.6840       489\n",
            "\n",
            "âœ“ Classification report saved to results/classification_report_Ensemble.txt\n",
            "\n",
            "======================================================================\n",
            "CREATING FEATURE IMPORTANCE PLOTS\n",
            "======================================================================\n",
            "âœ“ Feature importance plot saved to results/feature_importance.png\n",
            "\n",
            "======================================================================\n",
            "CREATING LEARNING CURVES\n",
            "======================================================================\n",
            "ðŸ”§ Computing learning curve for LightGBM...\n",
            "ðŸ”§ Computing learning curve for XGBoost...\n",
            "ðŸ”§ Computing learning curve for RandomForest...\n",
            "âœ“ Learning curves saved to results/learning_curves.png\n",
            "\n",
            "======================================================================\n",
            "CREATING ROC CURVES\n",
            "======================================================================\n",
            "âœ“ ROC curves saved to results/roc_curves_ensemble.png\n",
            "\n",
            "======================================================================\n",
            "CREATING PREDICTION DISTRIBUTION PLOTS\n",
            "======================================================================\n",
            "âœ“ Prediction distribution plot saved to results/prediction_distribution.png\n",
            "\n",
            "======================================================================\n",
            "SAVING MODELS AND ARTIFACTS\n",
            "======================================================================\n",
            "âœ“ Saved LightGBM model to results/model_lightgbm.pkl\n",
            "âœ“ Saved XGBoost model to results/model_xgboost.pkl\n",
            "âœ“ Saved RandomForest model to results/model_randomforest.pkl\n",
            "âœ“ Saved ExtraTrees model to results/model_extratrees.pkl\n",
            "âœ“ Saved LogisticRegression model to results/model_logisticregression.pkl\n",
            "âœ“ Saved scaler to results/scaler.pkl\n",
            "âœ“ Saved imputer to results/imputer.pkl\n",
            "âœ“ Saved label encoder to results/label_encoder.pkl\n",
            "âœ“ Saved feature info to results/feature_info.json\n",
            "âœ“ Saved feature scores to results/feature_scores.csv\n",
            "\n",
            "======================================================================\n",
            "PIPELINE COMPLETED SUCCESSFULLY!\n",
            "======================================================================\n",
            "\n",
            "ðŸ“Š FINAL MODEL RANKINGS:\n",
            "1. XGBoost              - Accuracy: 0.7219, F1: 0.6878\n",
            "2. Ensemble             - Accuracy: 0.7198, F1: 0.6840\n",
            "3. LightGBM             - Accuracy: 0.7157, F1: 0.6977\n",
            "4. ExtraTrees           - Accuracy: 0.7076, F1: 0.6386\n",
            "5. RandomForest         - Accuracy: 0.7035, F1: 0.6275\n",
            "6. LogisticRegression   - Accuracy: 0.5337, F1: 0.5809\n",
            "\n",
            "âœ“ All results saved to 'results/' directory\n",
            "âœ“ Models, scalers, and artifacts ready for production deployment\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m_QvoGLnQ-VL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Ð”Ð¾Ð±Ñ€Ð¾ Ð¿Ð¾Ð¶Ð°Ð»Ð¾Ð²Ð°Ñ‚ÑŒ Ð² Colab!",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}